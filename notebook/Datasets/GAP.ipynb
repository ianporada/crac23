{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, LongformerTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_file = \"/home/shtoshni/Research/litbank_coref/data/gap/orig/gap-test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-large-4096', add_prefix_space=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "data = open(gap_file).readlines()[1:]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test-1\tUpon their acceptance into the Kontinental Hockey League, Dehner left Finland to sign a contract in Germany with EHC M*nchen of the DEL on June 18, 2014. After capturing the German championship with the M*nchen team in 2016, he left the club and was picked up by fellow DEL side EHC Wolfsburg in July 2016. Former NHLer Gary Suter and Olympic-medalist Bob Suter are Dehner's uncles. His cousin is Minnesota Wild's alternate captain Ryan Suter.\tHis\t383\tBob Suter\t352\tFALSE\tDehner\t366\tTRUE\thttp://en.wikipedia.org/wiki/Jeremy_Dehner\n",
      "\n",
      "[[[84, 86], [94, 94]]]\n",
      "[[[88, 89]], [[95, 95]]]\n"
     ]
    }
   ],
   "source": [
    "instances = []\n",
    "for line in data:\n",
    "    doc_key, text, pronoun, pronoun_offset, span1, span1_offset, coref1, span2, span2_offset, coref2 = line.strip().split('\\t')[:10]\n",
    "    \n",
    "    pronoun_offset, span1_offset, span2_offset = int(pronoun_offset), int(span1_offset), int(span2_offset)\n",
    "    \n",
    "    pronoun_boundary = (pronoun_offset, pronoun_offset + len(pronoun))\n",
    "    span1_boundary = (span1_offset, span1_offset + len(span1))\n",
    "    span2_boundary = (span2_offset, span2_offset + len(span2))\n",
    "    \n",
    "    \n",
    "    print(line)\n",
    "    \n",
    "    for span_boundary, coref_label in zip([span1_boundary, span2_boundary], [coref1, coref2]):\n",
    "        boundaries = sorted([pronoun_boundary, span_boundary], key=lambda x: x[0])\n",
    "        \n",
    "        \n",
    "        first_span = text[0: boundaries[0][0]].strip()\n",
    "        second_span = text[boundaries[0][0]: boundaries[0][1]].strip()\n",
    "        third_span = text[boundaries[0][1]: boundaries[1][0]].strip()\n",
    "        fourth_span = text[boundaries[1][0]: boundaries[1][1]].strip()\n",
    "        fifth_span = text[boundaries[1][1]:].strip()\n",
    "        \n",
    "        \n",
    "        doc = []\n",
    "        prefix_len = []\n",
    "        spans = []\n",
    "        for idx, intermediate_span in enumerate([first_span, second_span, third_span, fourth_span, fifth_span]):\n",
    "            prefix_len.append(len(doc))\n",
    "            span_tokens = tokenizer.tokenize(intermediate_span)\n",
    "            if idx == 1 or idx == 3:\n",
    "                spans.append([prefix_len[-1], prefix_len[-1] + len(span_tokens) - 1])\n",
    "                \n",
    "#             doc.extend(span_tokens)\n",
    "            doc.extend(tokenizer.convert_tokens_to_ids(span_tokens))\n",
    "#             doc.extend(tokenizer.tokenize(intermediate_span))\n",
    "\n",
    "        \n",
    "        clusters = []\n",
    "        if coref_label == 'TRUE':\n",
    "            clusters = [[spans[0]], [spans[1]]]\n",
    "        else:\n",
    "            clusters = [[spans[0], spans[1]]]\n",
    "            \n",
    "        print(clusters)\n",
    "#         for span_start, span_end in spans:\n",
    "#             print(tokenizer.convert_tokens_to_string(doc[span_start: span_end + 1]))\n",
    "        \n",
    "#         print(tokenizer.convert_tokens_to_string(doc))\n",
    "        \n",
    "    \n",
    "    \n",
    "    break\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rap_nlp] *",
   "language": "python",
   "name": "conda-env-rap_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
