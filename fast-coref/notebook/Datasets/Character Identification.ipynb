{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "\n",
    "from transformers import LongformerTokenizerFast\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-large-4096', add_prefix_space=True)\n",
    "\n",
    "data_files = glob.glob(\"/home/shtoshni/Research/litbank_coref/data/character_identification/orig/character*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.117437337942956\n"
     ]
    }
   ],
   "source": [
    "mention_count = 0\n",
    "uniq_entities = set()\n",
    "\n",
    "from collections import Counter\n",
    "cluster_lens = []\n",
    "for data_file in data_files:\n",
    "    data = json.load(open(data_file))\n",
    "    for episode in data['episodes']:\n",
    "        \n",
    "        for scene in episode['scenes']:\n",
    "            doc_key = scene['scene_id']\n",
    "            general_counter = 0\n",
    "            \n",
    "            tokens = []\n",
    "            clusters = defaultdict(list)\n",
    "            subtokens_list = []\n",
    "            for utterance in scene['utterances']:\n",
    "                speakers = (utterance['speakers'])\n",
    "                entities_list = utterance['character_entities']\n",
    "    #             if len(speakers) > 1:\n",
    "    #                 print(speakers)\n",
    "                \n",
    "                utterance_clusters = defaultdict(list)\n",
    "                utterance_map = {}\n",
    "                \n",
    "                for idx, (sent, per_sent_entities) in enumerate(zip(utterance['tokens'], utterance['character_entities'])):\n",
    "                    sent_word_idx = 0\n",
    "                    sentence_token_map = {}\n",
    "                    \n",
    "                    for token_idx, token in enumerate(sent):\n",
    "                        sentence_token_map[token_idx] = [len(subtokens_list)]\n",
    "                        subtokens_list.extend(tokenizer.tokenize(token))\n",
    "                        sentence_token_map[token_idx].append(len(subtokens_list))\n",
    "                    \n",
    "                    for entity in per_sent_entities:\n",
    "                        characters  = tuple(sorted(entity[2:]))\n",
    "                        token_start, token_end = entity[:2]\n",
    "                        \n",
    "                        span_start = sentence_token_map[token_start][0]\n",
    "                        span_end = sentence_token_map[token_end - 1][1] - 1\n",
    "                        utterance_clusters[characters].append((span_start, span_end))\n",
    "#                         for character in characters:\n",
    "#                             utterance_clusters[character] += 1\n",
    "#                         uniq_entities.update(characters)\n",
    "                \n",
    "                for character in utterance_clusters:\n",
    "#                     clusters[character] += utterance_clusters[character]\n",
    "                    if character != '#GENERAL#':\n",
    "                        clusters[character].extend(utterance_clusters[character])\n",
    "                    else:\n",
    "                        clusters[character + str(general_counter)] = utterance_clusters[character]\n",
    "                        general_counter += 1\n",
    "                        \n",
    "                        \n",
    "            \n",
    "            cluster_lens.extend([len(cluster) for cluster in clusters.values()])\n",
    "            \n",
    "#             print(clusters)\n",
    "#             print(tokenizer.convert_tokens_to_string(subtokens_list))\n",
    "#             for person, ment_list in clusters.items():\n",
    "#                 print(person)\n",
    "#                 for ment_start, ment_end in ment_list:\n",
    "#                     print(tokenizer.convert_tokens_to_string(subtokens_list[ment_start: ment_end + 1]))\n",
    "                \n",
    "#             break\n",
    "        \n",
    "#         break\n",
    "#     break\n",
    "\n",
    "\n",
    "print(sum(cluster_lens)/len(cluster_lens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rap_nlp] *",
   "language": "python",
   "name": "conda-env-rap_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
