{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from collections import Counter\n",
    "\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from transformers import BertTokenizer, LongformerTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TRANSFORMERS_OFFLINE']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LongformerTokenizerFast.from_pretrained('allenai/longformer-large-4096', add_prefix_space=True)\n",
    "\n",
    "wsc_file = \"/home/shtoshni/Research/coref_resources/data/wsc/WSCollection.xml\"\n",
    "len_limit = 273\n",
    "\n",
    "tree = ET.parse(wsc_file)\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "285\n"
     ]
    }
   ],
   "source": [
    "print(len(list(root))) \n",
    "## 12 additional examples have been added to the original 273"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_span(word_list, token_list):\n",
    "    for start_idx in range(0, len(word_list) - len(token_list) + 1):\n",
    "        match = start_idx\n",
    "        for token1, token2 in zip(word_list[start_idx: start_idx + len(token_list)], token_list):\n",
    "            if token1 != token2:\n",
    "                match = -1\n",
    "                break\n",
    "        \n",
    "#         print(word_list, token_list, match)\n",
    "\n",
    "        if match == -1:\n",
    "            continue\n",
    "        else:\n",
    "            return match\n",
    "        \n",
    "    return -1\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546\n",
      "546\n",
      "273\n",
      "The city councilmen refused the demonstrators a permit because they feared violence.\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "prefixes = []\n",
    "pronouns = []\n",
    "continuations = []\n",
    "\n",
    "answers = []\n",
    "correct_answers = []\n",
    "\n",
    "for elem in list(root)[:len_limit]:\n",
    "    for children in list(elem.iter('txt1')):\n",
    "        prefixes.append(children.text.strip().replace('\\n', ' '))\n",
    "\n",
    "    for children in list(elem.iter('pron')):\n",
    "        pronouns.append(children.text.strip())\n",
    "\n",
    "    for children in list(elem.iter('txt2')):\n",
    "        continuations.append(children.text.strip())\n",
    "\n",
    "    for children in list(elem.iter('answer')):\n",
    "        answers.append(children.text.strip())\n",
    "\n",
    "\n",
    "    for children in list(elem.iter('correctAnswer')):\n",
    "        correct_answers.append(children.text.strip()[0])\n",
    "\n",
    "    \n",
    "print(len(answers))\n",
    "print(len(pronouns))\n",
    "print(len(prefixes))\n",
    "\n",
    "\n",
    "print(f'{prefixes[0]} {pronouns[0]} {continuations[0]}')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ĠThe', 'Ġcity', 'Ġcouncil', 'men', 'Ġrefused', 'Ġthe', 'Ġdemonstrators', 'Ġa', 'Ġpermit', 'Ġbecause', 'Ġthey', 'Ġfeared', 'Ġviolence', '.']\n",
      "[10, 10]\n",
      "[[[0, 3]], [[5, 6]]]\n",
      "[[[0, 3], [10, 10]], [[5, 6]]]\n",
      "['ĠThe', 'Ġcity', 'Ġcouncil', 'men']\n",
      "['Ġthey']\n",
      "['Ġthe', 'Ġdemonstrators']\n",
      "['Ġthey']\n",
      "['ĠThe', 'Ġcity', 'Ġcouncil', 'men', 'Ġrefused', 'Ġthe', 'Ġdemonstrators', 'Ġa', 'Ġpermit', 'Ġbecause', 'Ġthey', 'Ġadvocated', 'Ġviolence', '.']\n",
      "[10, 10]\n",
      "[[[0, 3]], [[5, 6]]]\n",
      "[[[0, 3]], [[5, 6], [10, 10]]]\n",
      "['ĠThe', 'Ġcity', 'Ġcouncil', 'men']\n",
      "['Ġthe', 'Ġdemonstrators']\n",
      "['Ġthey']\n",
      "['Ġthey']\n",
      "['ĠThe', 'Ġtrophy', 'Ġdoesn', \"'t\", 'Ġfit', 'Ġinto', 'Ġthe', 'Ġbrown', 'Ġsuitcase', 'Ġbecause', 'Ġit', 'Ġis', 'Ġtoo', 'Ġlarge', '.']\n",
      "[10, 10]\n",
      "[[[0, 1]], [[6, 8]]]\n",
      "[[[0, 1], [10, 10]], [[6, 8]]]\n",
      "['ĠThe', 'Ġtrophy']\n",
      "['Ġit']\n",
      "['Ġthe', 'Ġbrown', 'Ġsuitcase']\n",
      "['Ġit']\n",
      "['ĠThe', 'Ġtrophy', 'Ġdoesn', \"'t\", 'Ġfit', 'Ġinto', 'Ġthe', 'Ġbrown', 'Ġsuitcase', 'Ġbecause', 'Ġit', 'Ġis', 'Ġtoo', 'Ġsmall', '.']\n",
      "[10, 10]\n",
      "[[[0, 1]], [[6, 8]]]\n",
      "[[[0, 1]], [[6, 8], [10, 10]]]\n",
      "['ĠThe', 'Ġtrophy']\n",
      "['Ġthe', 'Ġbrown', 'Ġsuitcase']\n",
      "['Ġit']\n",
      "['Ġit']\n",
      "['ĠJoan', 'Ġmade', 'Ġsure', 'Ġto', 'Ġthank', 'ĠSusan', 'Ġfor', 'Ġall', 'Ġthe', 'Ġhelp', 'Ġshe', 'Ġhad', 'Ġrec', 'ieved', '.']\n",
      "[10, 10]\n",
      "[[[0, 0]], [[5, 5]]]\n",
      "[[[0, 0], [10, 10]], [[5, 5]]]\n",
      "['ĠJoan']\n",
      "['Ġshe']\n",
      "['ĠSusan']\n",
      "['Ġshe']\n",
      "['ĠJoan', 'Ġmade', 'Ġsure', 'Ġto', 'Ġthank', 'ĠSusan', 'Ġfor', 'Ġall', 'Ġthe', 'Ġhelp', 'Ġshe', 'Ġhad', 'Ġgiven', '.']\n",
      "[10, 10]\n",
      "[[[0, 0]], [[5, 5]]]\n",
      "[[[0, 0]], [[5, 5], [10, 10]]]\n",
      "['ĠJoan']\n",
      "['ĠSusan']\n",
      "['Ġshe']\n",
      "['Ġshe']\n",
      "Pam's parents came home and found her having sex with her boyfriend, Paul. They were furious about it. Pam and Paul\n",
      "Pam's parents came home and found her having sex with her boyfriend, Paul. They were embarrassed about it. Pam and Paul\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "not_found_count = 0\n",
    "for idx, prefix in enumerate(prefixes):\n",
    "    answer1 = answers[idx * 2]\n",
    "    answer2 = answers[idx * 2 + 1]\n",
    "    \n",
    "    text = f'{prefix} {pronouns[idx * 2]} {continuations[idx]}'\n",
    "    word_list = tokenizer.tokenize(prefix)\n",
    "    prefix_idx = len(word_list)\n",
    "    word_list += tokenizer.tokenize(pronouns[idx * 2])\n",
    "    \n",
    "    pronoun_boundary = [prefix_idx, len(word_list) - 1]\n",
    "    word_list += tokenizer.tokenize(continuations[idx])\n",
    "    \n",
    "    answer_boundaries = []\n",
    "    \n",
    "    for answer in [answer1, answer2]:\n",
    "        for span in [answer, answer.lower(), answer.capitalize()]:\n",
    "            span_tokens = tokenizer.tokenize(span)\n",
    "            found = search_span(word_list, span_tokens)\n",
    "            if found != -1:\n",
    "                answer_boundaries.append([[found, found + len(span_tokens) - 1]])\n",
    "                break\n",
    "                \n",
    "        if found == -1:        \n",
    "            print(text, answer)\n",
    "            not_found_count += 1\n",
    "            \n",
    "    \n",
    "    import copy\n",
    "    clusters = copy.deepcopy(answer_boundaries)\n",
    "    if len(answer_boundaries) == 2:\n",
    "        correct_answer = correct_answers[idx]\n",
    "        assert (correct_answer in ['A', 'B'])\n",
    "        \n",
    "        if correct_answer == 'A':\n",
    "            clusters[0].append(pronoun_boundary)\n",
    "        else:\n",
    "            clusters[1].append(pronoun_boundary)\n",
    "        \n",
    "        \n",
    "        \n",
    "    if idx <= 5:\n",
    "        print(word_list)\n",
    "        \n",
    "        print(pronoun_boundary)\n",
    "        print(answer_boundaries)\n",
    "        print(clusters)\n",
    "        \n",
    "        all_boundaries = flatten(clusters)\n",
    "        all_boundaries.append(pronoun_boundary)\n",
    "        \n",
    "        for ment_start, ment_end in all_boundaries:\n",
    "            print(word_list[ment_start: ment_end + 1])\n",
    "\n",
    "    \n",
    "#     break\n",
    "\n",
    "print(not_found_count)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rap_nlp] *",
   "language": "python",
   "name": "conda-env-rap_nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
